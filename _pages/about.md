---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

My name is Zhihan Zhang (张智涵). I am a third-year Ph.D. student in Computer Science and Engineering at the [University of Notre Dame](https://www.nd.edu/), advised by [Dr. Meng Jiang](http://www.meng-jiang.com/). I am currently a member of Dr. Jiang's [Data Mining towards Decision Making](http://www.meng-jiang.com/lab.html) (DM<sup>2</sup>) Lab. Prior to my Ph.D. career, I received my Bachelor's degree in Computer Science at [Peking University](https://www.pku.edu.cn/), where I had the fortune of working with [Dr. Yunfang Wu](https://cs.pku.edu.cn/info/1237/2096.htm) and [Dr. Xu Sun](https://xusun.org/). My research is mainly related to the interaction between *natural language instructions* and *language models*, such as instruction tuning and the evaluation of instruction-following language models.

For my past education and research experience, please refer to [Experience](http://ytyz1307zzh.github.io/experience). For the full list of my publications, please refer to [Publications](http://ytyz1307zzh.github.io/publications) or check my [Google Scholar page](https://scholar.google.com/citations?user=7dcunDUAAAAJ&hl=en).

News
======
-  <img src="../images/new.png" width="25" align=center> Jun 2024: Several new preprints came out! Check out our latest works:
    * A [first-authored paper](https://arxiv.org/abs/2406.12050) worked on fine-tuning language models for mathematical reasoning. By augmenting training examples with reflective data, we improved the math abilities of models in both the standard single-round QA and more complex reflective reasoning scenarios.
    * A [co-authored paper](https://arxiv.org/abs/2405.14092) studied the problem of language models self-correcting their own predictions. We utilized backward verification methods to unleash the model's capacity to identify and correct their reasoning errors without external feedback.
    * A [co-authored paper](https://arxiv.org/abs/2405.19444) proposed a new benchmark for comprehensively evaluating the mathematical capabilities of language models. We went beyond single-round QA and assessed the models' math abilities in multi-turn interactions and open ended generative scenarios.
    * A [co-authored paper](https://arxiv.org/abs/2404.14604) studied multi-modal mathematical reasoning. We focused on improving MLLMs' visual understanding capacity on math figures via explicit visual comprehension training, which improved their math reasoning accuracy.
-  Jun 2024: I am joining Amazon as a full-time research intern in summer 2024.
-  May 2024: One [first-authored paper](https://arxiv.org/abs/2311.08711) is accepted to **ACL 2024** main conference! We studied the challenges in cross-lingual instruction-tuning and proposed a simple yet effective method to improve LLMs' proficiency in low-resource languages. Code is available on [Github](https://github.com/ytyz1307zzh/PLUG).
-  Feb 2023: I am joining Tecent America as a full-time research intern in spring 2024.
-  Oct 2023: Two papers are accepted to **EMNLP 2023**! These include one [first-authored paper](https://arxiv.org/abs/2310.13127) on automatic instruction optimization of LLMs, and one [co-authored paper](https://arxiv.org/abs/2305.14457) on language model pre-training for comparative reasoning.

Contact
======
- Email: zzhang23 [at] nd.edu
- Office: 355 Fitzpatrick Hall of Engineering
- Location: University of Notre Dame, Notre Dame, IN 46556
